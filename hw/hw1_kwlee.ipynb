{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1\n",
    "\n",
    "Department of Statistics, Kyeongwon Lee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- #### Transiting to Backpropagation\n",
    "- Let's go back to our simple FNN to put things in perspective\n",
    "    - Let us ignore non-linearities for now to keep it simpler, but it's just a tiny change subsequently\n",
    "    - Given a linear transformation on our input (for simplicity instead of an affine transformation that includes a bias): $\\hat y = \\theta x$\n",
    "        - $\\theta$ is our parameters\n",
    "        - $x$ is our input\n",
    "        - $\\hat y$ is our prediction\n",
    "    - Then we have our MSE loss function $L = \\frac{1}{2} (\\hat y - y)^2$\n",
    "- We need to calculate our partial derivatives of our loss w.r.t. our parameters to update our parameters: $\\nabla_{\\theta} = \\frac{\\delta L}{\\delta \\theta}$\n",
    "    - With chain rule we have $\\frac{\\delta L}{\\delta \\theta} = \\frac{\\delta L}{\\delta \\hat y} \\frac{\\delta \\hat y}{\\delta \\theta}$\n",
    "        - $\\frac{\\delta L}{\\delta \\hat y} = (\\hat y -  y)$\n",
    "        - $\\frac{\\delta \\hat y}{\\delta \\theta}$ is our partial derivatives of $y$ w.r.t. our parameters (our gradient) as we have covered previously\n",
    "        \n",
    " -->\n",
    "<!-- #### Forward Propagation, Backward Propagation and Gradient Descent -->\n",
    "## Model\n",
    "<!-- - All right, now let's put together what we have learnt on backpropagation and apply it on a simple feedforward neural network (FNN)\n",
    "- Let us assume the following simple FNN architecture and take note that we do not have bias here to keep things simple -->\n",
    "\n",
    "- FNN architecture\n",
    "    1. Linear function: hidden size = 32\n",
    "    2. Non-linear function: sigmoid\n",
    "    3. Linear function: output size = <font color=\"red\">16 </font>\n",
    "    4. Non-linear function: sigmoid\n",
    "    5. <font color=\"red\">Linear function: output size = 1 </font>\n",
    "    6. <font color=\"red\">Non-linear function: sigmoid </font>\n",
    "\n",
    "- We will be going through a binary classification problem classifying 2 types of flowers\n",
    "    - Output size: 1 (represented by 0 or 1 depending on the flower)\n",
    "    - Input size: 2 (features of the flower)\n",
    "\n",
    "![](https://www.deeplearningwizard.com/deep_learning/boosting_models_pytorch/images/manual_bp.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I add one hidden layer to original FNN, i.e. \n",
    "$$\n",
    "X_1 \\xrightarrow{W_1*\\cdot} Y_1 \\xrightarrow{\\sigma(\\cdot)} Y_2 \\xrightarrow{W_2*\\cdot} Y_3 \\xrightarrow{\\sigma(\\cdot)} Y_4 \\xrightarrow{W_3*\\cdot} Y_5 \\xrightarrow{\\sigma(\\cdot)} Y_6,\n",
    "$$\n",
    "and correct the last derivate\n",
    "$$\n",
    "    \\frac{d C}{d Y_6} = -\\frac{L}{Y_6} + \\frac{1-L}{1-Y_6} = \\frac{Y_6 - L}{Y_6 (1-Y_6)} \\neq Y_6 - L = \\frac{d}{dY_6} \\left\\{ \\frac{1}{2}(Y_6 - L)^2 \\right\\}.\n",
    "$$\n",
    "\n",
    "Note partial derivatives of cost are\n",
    "\\begin{align*}\n",
    "    \\frac{dC}{dW_3} &= \\frac{dC}{dY_5}\\frac{dY_5}{dW_3} = \\frac{dC}{dY_6}\\frac{dY_6}{dY_5}\\frac{dY_5}{dW_3}, \\\\\n",
    "    \\frac{dC}{dW_2} &= \\frac{dC}{dY_3}\\frac{dY_3}{dW_2} = \\frac{dC}{dY_6}\\frac{dY_6}{dY_5}\\frac{dY_5}{dY_4}\\frac{dY_4}{dY_3}\\frac{dY_3}{dW_2}, \\\\\n",
    "    \\frac{dC}{dW_1} &= \\frac{dC}{dY_1}\\frac{dY_1}{dW_1} = \\frac{dC}{dY_6}\\frac{dY_6}{dY_5}\\frac{dY_5}{dY_4}\\frac{dY_4}{dY_3}\\frac{dY_3}{dY_2}\\frac{dY_2}{dY_1}\\frac{dY_1}{dW_1}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "iris = datasets.load_iris()\n",
    "X = torch.tensor(preprocessing.normalize(iris.data[:, :2]), dtype=torch.float)\n",
    "y = torch.tensor(iris.target.reshape(-1, 1), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x12273a650>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set manual seed\n",
    "torch.manual_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([150, 2])\n",
      "torch.Size([150, 1])\n"
     ]
    }
   ],
   "source": [
    "print(X.size())\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only take 2 classes to make a binary classification problem\n",
    "X = X[:y[y < 2].size()[0]]\n",
    "y = y[:y[y < 2].size()[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 2])\n",
      "torch.Size([100, 1])\n"
     ]
    }
   ],
   "source": [
    "print(X.size())\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified model\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Dimensions for input, hidden and output\n",
    "        self.input_dim = 2\n",
    "        self.hidden_dim = 32\n",
    "        self.hidden2_dim = 16\n",
    "        self.output_dim = 1\n",
    "        \n",
    "        # Learning rate definition\n",
    "        self.learning_rate = 0.001\n",
    "        \n",
    "        # Our parameters (weights)\n",
    "        # w1: 2 x 32\n",
    "        self.w1 = torch.randn(self.input_dim, self.hidden_dim)\n",
    "        \n",
    "        # w2: 32 x 16\n",
    "        self.w2 = torch.randn(self.hidden_dim, self.hidden2_dim)\n",
    "        \n",
    "        # w3: 16 x 1\n",
    "        self.w3 = torch.randn(self.hidden2_dim, self.output_dim)\n",
    "    \n",
    "    \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1 + torch.exp(-s))\n",
    "    \n",
    "    def sigmoid_first_order_derivative(self, s):\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    # Forward propagation\n",
    "    def forward(self, X):\n",
    "        # First linear layer\n",
    "        self.y1 = torch.matmul(X, self.w1) # 3 X 3 \".dot\" does not broadcast in PyTorch\n",
    "        \n",
    "        # First non-linearity\n",
    "        self.y2 = self.sigmoid(self.y1)\n",
    "        \n",
    "        # Second linear layer\n",
    "        self.y3 = torch.matmul(self.y2, self.w2)\n",
    "        \n",
    "        # Second non-linearity\n",
    "        self.y4 = self.sigmoid(self.y3)\n",
    "        \n",
    "        #### Add the third hidden layer\n",
    "        \n",
    "        # Third linear layer\n",
    "        self.y5 = torch.matmul(self.y4, self.w3)\n",
    "        \n",
    "        # Second non-linearity\n",
    "        y6 = self.sigmoid(self.y5)\n",
    "        \n",
    "        return y6\n",
    "        \n",
    "    # Backward propagation\n",
    "    def backward(self, X, l, y6):\n",
    "        # Derivative of binary cross entropy cost w.r.t. final output y6\n",
    "        \n",
    "        self.dC_dy6 = (y6 - l)/(y6 * (1-y6)) # corrected loss\n",
    "\n",
    "        \n",
    "        '''\n",
    "        Gradients for w3: partial derivative of cost w.r.t. w3\n",
    "        dC/dw3\n",
    "        '''\n",
    "        self.dy6_dy5 = self.sigmoid_first_order_derivative(y6)\n",
    "        self.dy5_dw3 = self.y4\n",
    "        \n",
    "        # dC_dy5\n",
    "        self.y6_delta = self.dC_dy6 * self.dy6_dy5 \n",
    "        \n",
    "        # gradients for w3\n",
    "        self.dC_dw3 = torch.matmul(torch.t(self.dy5_dw3), self.y6_delta)\n",
    "        \n",
    "        '''\n",
    "        Gradients for w2: partial derivative of cost w.r.t. w2\n",
    "        dC/dw2\n",
    "        '''\n",
    "        self.dy5_dy4 = self.w3\n",
    "        self.dy4_dy3 = self.sigmoid_first_order_derivative(self.y4)\n",
    "        \n",
    "        # Y4 delta\n",
    "        # dC_dy3 = dC_dy5 * dy5_dy4 * dy4_dy3\n",
    "        self.y4_delta = torch.matmul(self.y6_delta, torch.t(self.dy5_dy4)) * self.dy4_dy3\n",
    "        self.dy3_dw2 = self.y2\n",
    "        # gradients for w2\n",
    "        self.dC_dw2 = torch.matmul(torch.t(self.dy3_dw2), self.y4_delta)\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        Gradients for w1: partial derivative of cost w.r.t w1\n",
    "        dC/dw1\n",
    "        '''\n",
    "        self.dy3_dy2 = self.w2\n",
    "        self.dy2_dy1 = self.sigmoid_first_order_derivative(self.y2)\n",
    "        \n",
    "        # Y2 delta: (dC_dy4 dy4_dy3) dy3_dy2 dy2_dy1\n",
    "        self.y2_delta = torch.matmul(self.y4_delta, torch.t(self.dy3_dy2)) * self.dy2_dy1\n",
    "        \n",
    "        # Gradients for w1: (dC_dy4 dy4_dy3) dy3_dy2 dy2_dy1 dy1_dw1\n",
    "        self.dC_dw1 = torch.matmul(torch.t(X), self.y2_delta)\n",
    "\n",
    "        \n",
    "        # Gradient descent on the weights from our 2 linear layers\n",
    "        self.w1 -= self.learning_rate * self.dC_dw1\n",
    "        self.w2 -= self.learning_rate * self.dC_dw2\n",
    "        self.w3 -= self.learning_rate * self.dC_dw3\n",
    "\n",
    "    def train(self, X, l):\n",
    "        # Forward propagation\n",
    "        y6 = self.forward(X)\n",
    "        \n",
    "        # Backward propagation and gradient descent\n",
    "        self.backward(X, l, y6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss: 1.1172503232955933\n",
      "Epoch 20 | Loss: 0.6934790015220642\n",
      "Epoch 40 | Loss: 0.6900936961174011\n",
      "Epoch 60 | Loss: 0.6867055296897888\n",
      "Epoch 80 | Loss: 0.6833014488220215\n",
      "Epoch 100 | Loss: 0.6798701286315918\n"
     ]
    }
   ],
   "source": [
    "# Instantiate our model class and assign it to our model object\n",
    "model = FNN()\n",
    "\n",
    "# Loss list for plotting of loss behaviour\n",
    "loss_lst = []\n",
    "\n",
    "# Number of times we want our FNN to look at all 100 samples we have, 100 implies looking through 100x\n",
    "num_epochs = 101\n",
    "\n",
    "# Let's train our model with 100 epochs\n",
    "for epoch in range(num_epochs):\n",
    "    # Get our predictions\n",
    "    y_hat = model(X)\n",
    "    \n",
    "    # Cross entropy loss, remember this can never be negative by nature of the equation\n",
    "    # But it does not mean the loss can't be negative for other loss functions\n",
    "    cross_entropy_loss = -(y * torch.log(y_hat) + (1 - y) * torch.log(1 - y_hat))\n",
    "    \n",
    "    # We have to take cross entropy loss over all our samples, 100 in this 2-class iris dataset\n",
    "    mean_cross_entropy_loss = torch.mean(cross_entropy_loss).detach().item()\n",
    "    \n",
    "    # Print our mean cross entropy loss\n",
    "    if epoch % 20 == 0:\n",
    "        print('Epoch {} | Loss: {}'.format(epoch, mean_cross_entropy_loss))\n",
    "    loss_lst.append(mean_cross_entropy_loss)\n",
    "    \n",
    "    # (1) Forward propagation: to get our predictions to pass to our cross entropy loss function\n",
    "    # (2) Back propagation: get our partial derivatives w.r.t. parameters (gradients)\n",
    "    # (3) Gradient Descent: update our weights with our gradients\n",
    "    model.train(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_lst, label = \"loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}