{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KorBERT_NSMC\n",
    "\n",
    "Department of Statistics, Kyeongwon Lee\n",
    "\n",
    "* 발표자료는 [github repository](https://github.com/kw-lee/2019compling1/blob/master/KoBERT_NSMC/Presentation.ipynb)에서 확인하실 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environments\n",
    "\n",
    "* OS: Ubuntu 16.04.5 LTS x86_64\n",
    "* CPU: Intel Xeon Silver 4110 (32) @ 2.095GHz\n",
    "* GPU: Nvidia TITAN Xp (use only 1 GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* aisolab: https://github.com/aisolab/nlp_implementation\n",
    "* [KorBERT_WordPiece](http://aiopen.etri.re.kr/service_dataset.php) from ETRI\n",
    "    * 학습데이터: 23GB 원시 말뭉치\n",
    "    * 딥러닝 라이브러리: pytorch, tensorflow\n",
    "    * 소스코드: tokenizer\n",
    "    * Latin alphabets: Cased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고: \n",
    "\n",
    "* `bert_eojeol_pytorch/src_tokenizer/tokenization.py`의 `.file_utils`를 `transformers.file_utils`로 변경해야합니다.\n",
    "* `bert_eojeol_pytorch/bert_config.json`의 맨 마지막에 `\"layer_norm_eps\": 1e-12`를 추가해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q transformers tqdm future\n",
    "# !git clone https://github.com/aisolab/nlp_implementation\n",
    "# !mkdir aisolab\n",
    "# !cp -r nlp_implementation/BERT_single_sentence_classification/* ./aisolab\n",
    "# !rm -rf nlp_implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers.modeling_bert import BertConfig, BertPreTrainedModel, BertModel\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "import json\n",
    "\n",
    "import bert_eojeol_pytorch.src_tokenizer.tokenization as tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from aisolab\n",
    "from aisolab.model.data import Corpus # easier preprocessing\n",
    "from aisolab.model.utils import PreProcessor, PadSequence, Vocab # easier preprocessing\n",
    "from aisolab.model.metric import evaluate, acc # metric\n",
    "from aisolab.utils import Config, CheckpointManager, SummaryManager # easier configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Using the [Naver sentiment movie corpus](https://github.com/e9t/nsmc) (a.k.a. NSMC)\n",
    "\n",
    "```bash\n",
    "    wget -nc -q -O data/train.tsv https://www.dropbox.com/s/374ftkec978br3d/ratings_train.txt?dl=1\n",
    "    wget -nc -q -O data/test.tsv https://www.dropbox.com/s/977gbwh542gdy94/ratings_test.txt?dl=1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# loading dataset\n",
    "data_dir = Path(\"data\")\n",
    "filepath = data_dir / \"train.tsv\"\n",
    "\n",
    "dataset = pd.read_csv(filepath, sep=\"\\t\").loc[:, [\"document\", \"label\"]]\n",
    "dataset = dataset.loc[dataset[\"document\"].isna().apply(lambda elm: not elm), :]\n",
    "tr, val = train_test_split(dataset, test_size=0.2, random_state=777)\n",
    "\n",
    "tr.to_csv(data_dir / \"train.txt\", sep=\"\\t\", index=False)\n",
    "val.to_csv(data_dir / \"validation.txt\", sep=\"\\t\", index=False)\n",
    "\n",
    "tst_filepath = data_dir / \"test.tsv\"\n",
    "tst = pd.read_csv(tst_filepath, sep=\"\\t\").loc[:, [\"document\", \"label\"]]\n",
    "tst = tst.loc[tst[\"document\"].isna().apply(lambda elm: not elm), :]\n",
    "tst.to_csv(data_dir / \"test.txt\", sep=\"\\t\", index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/train.tsv', sep='\\t')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = df['label'].value_counts().plot.bar(rot = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('data')\n",
    "model_dir = Path('model')\n",
    "ptr_dir = Path('bert_eojeol_pytorch') # downloaded frem ETRI\n",
    "ptr_config_path = ptr_dir / 'bert_config.json'\n",
    "ptr_tokenizer_path = ptr_dir / \"vocab.korean.rawtext.list\"\n",
    "ptr_bert_path = ptr_dir / \"pytorch_model.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptr_tokenizer = tokenization.BertTokenizer.from_pretrained(\n",
    "    ptr_tokenizer_path, do_lower_case=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate `vocab` (vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_token = list(ptr_tokenizer.vocab.keys())\n",
    "token_to_idx = {token: idx for idx, token in enumerate(idx_to_token)}\n",
    "\n",
    "vocab = Vocab(\n",
    "    idx_to_token,\n",
    "    padding_token=\"[PAD]\",\n",
    "    unknown_token=\"[UNK]\",\n",
    "    bos_token=None,\n",
    "    eos_token=None,\n",
    "    reserved_tokens=[\"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
    "    token_to_idx=token_to_idx,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save `vocab`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptr_vocab_path = ptr_dir / \"vocab_etri.pkl\"\n",
    "with open(ptr_vocab_path, mode=\"wb\") as io:\n",
    "    pickle.dump(vocab, io)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = Config(\n",
    "    {\n",
    "        \"train\": str(data_dir / \"train.txt\"),\n",
    "        \"validation\": str(data_dir / \"validation.txt\"),\n",
    "        \"test\": str(data_dir / \"test.txt\"),\n",
    "    }\n",
    ")\n",
    "data_config.save(data_dir / \"config.json\")\n",
    "\n",
    "ptr_config = Config({'config': str(ptr_config_path),\n",
    "                     'bert': str(ptr_bert_path),\n",
    "                     'tokenizer': str(ptr_tokenizer_path),\n",
    "                     'vocab': str(ptr_vocab_path)})\n",
    "ptr_config.save(ptr_dir / \"config.json\")\n",
    "\n",
    "model_config = Config({\n",
    "  \"num_classes\": len(df['label'].value_counts()),\n",
    "  \"length\": 64,\n",
    "  \"epochs\": 5,\n",
    "  \"batch_size\": 128,\n",
    "  \"learning_rate\": 1e-3,\n",
    "  \"summary_step\": 500,\n",
    "})\n",
    "model_config.save(model_dir / \"config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.to_indices(vocab.padding_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* padding:\n",
    "    * `aisolab.model.utils`의 `PadSequence()` 함수를 이용\n",
    "    * padding length = 64\n",
    "    * padding value = 0\n",
    "* tokenizer:\n",
    "    * `aisolab.model.utils`의 `PreProcessor()` 함수를 이용\n",
    "    * `ptr_tokenizer.tokenize`를 이용해 tokenize, `vacab` 단어에 맞게 mapping, `pad_sequence`대로 pad_sequence를 붙이기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_sequence = PadSequence(\n",
    "    length = model_config.length, \n",
    "    pad_val = vocab.to_indices(vocab.padding_token))\n",
    "preprocessor = PreProcessor(\n",
    "    vocab = vocab, \n",
    "    split_fn = ptr_tokenizer.tokenize, \n",
    "    pad_fn = pad_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,    67,    18,  3304,    12,  1967,  3800,     8,  3304,    18,\n",
       "           591,   193,  2040,   141,  1104, 12747,     3,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [    2,    35,    64,   212,   504,    11,  1041,  1325,  4185,   204,\n",
       "           631,    40,   434,   598,   113,    53,  3740,    33,    42,     7,\n",
       "            60,  1568,   466,  6923,  3559,    62,   804,     9,   555,  4832,\n",
       "           255,     3,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_test = torch.tensor([preprocessor.preprocess('신은 인간을 만들었고, 인간은 클레멘타인을 만들었다'), \n",
    "              preprocessor.preprocess('사상최악의 영화! 빵점줄수없는게 아쉽다.. 보다가 돌렸다 그냥 어휴 눈썩어')])\n",
    "preprocess_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_example = [vocab.to_tokens(preprocess_test[0][i]) for i in range(len(preprocess_test[0]))]\n",
    "for i in good_example:\n",
    "    print(i, end=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_example = [vocab.to_tokens(preprocess_test[1][i]) for i in range(len(preprocess_test[1]))]\n",
    "for i in bad_example:\n",
    "    print(i, end=\", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`token` -> `Pretrained-Model` -> `ReLU` with dropout -> `FC` -> `softmax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig(ptr_config.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"finetuning_task\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"num_labels\": 2,\n",
       "  \"output_attentions\": false,\n",
       "  \"output_hidden_states\": false,\n",
       "  \"output_past\": true,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"pruned_heads\": {},\n",
       "  \"torchscript\": false,\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_bfloat16\": false,\n",
       "  \"vocab_size\": 30797\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from aisolab/model/net.py\n",
    "class SentenceClassifier(BertPreTrainedModel):\n",
    "    def __init__(self, config, num_classes, vocab) -> None:\n",
    "        super(SentenceClassifier, self).__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.linear = nn.Linear(config.hidden_size, num_classes)\n",
    "        self.vocab = vocab\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "    \n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        attention_mask = input_ids.ne(self.vocab.to_indices(self.vocab.padding_token)).float()\n",
    "        _, pooled_output = self.bert(input_ids=input_ids, attention_mask = attention_mask)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        relu_output = self.relu(pooled_output)\n",
    "        logits = self.linear(relu_output) # before sigmoid\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility\n",
    "torch.manual_seed(777)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_ds = Corpus(data_config.train, preprocessor.preprocess)\n",
    "tr_dl = DataLoader(tr_ds, batch_size=model_config.batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
    "val_ds = Corpus(data_config.validation, preprocessor.preprocess)\n",
    "val_dl = DataLoader(val_ds, batch_size=model_config.batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceClassifier(\n",
    "    config, \n",
    "    num_classes = model_config.num_classes,\n",
    "    vocab = preprocessor.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['linear.weight', 'linear.bias'], unexpected_keys=['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_pretrained = torch.load(ptr_config.bert)\n",
    "model.load_state_dict(bert_pretrained, strict = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "* loss = CrossEntropyLoss\n",
    "* Adam with learning rate\n",
    "    * pretrained model: `lr` / 100\n",
    "    * user-defined model: `lr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.001\n"
     ]
    }
   ],
   "source": [
    "print('lr: {}'.format(model_config.learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(\n",
    "    [\n",
    "        {\"params\": model.bert.parameters(), \"lr\": model_config.learning_rate / 100},\n",
    "        {\"params\": model.linear.parameters(), \"lr\": model_config.learning_rate}\n",
    "    ], \n",
    "    weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# writer = SummaryWriter('{}/runs_{}'.format(model_dir, 'ETRI'))\n",
    "checkpoint_manager = CheckpointManager(model_dir)\n",
    "summary_manager = SummaryManager(model_dir)\n",
    "best_val_loss = 1e+10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e1eac275c334d5b8b0bef71886f5040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epochs', max=5, style=ProgressStyle(description_width='initia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6322c8bc32549879f2b86e8de4a7f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='steps', max=937, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "steps: 100%|██████████| 235/235 [00:48<00:00,  4.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\r",
      "global_step:   0, tr_loss: 0.684, val_loss: 0.710, tr_acc: 0.570, val_acc: 0.497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "steps: 100%|██████████| 235/235 [00:49<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step: 500, tr_loss: 0.347, val_loss: 0.285, tr_acc: 0.845, val_acc: 0.882\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00d49da3652c4cabac1d7dbef59b58f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='steps', max=937, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "steps: 100%|██████████| 235/235 [00:49<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\r",
      "global_step: 1000, tr_loss: 0.248, val_loss: 0.266, tr_acc: 0.900, val_acc: 0.889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "steps: 100%|██████████| 235/235 [00:49<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step: 1500, tr_loss: 0.249, val_loss: 0.276, tr_acc: 0.897, val_acc: 0.889\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f60f8e662b04fb69837eab75c0c4ff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='steps', max=937, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "steps: 100%|██████████| 235/235 [00:52<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\r",
      "global_step: 2000, tr_loss: 0.198, val_loss: 0.255, tr_acc: 0.920, val_acc: 0.896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "steps: 100%|██████████| 235/235 [00:49<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step: 2500, tr_loss: 0.205, val_loss: 0.246, tr_acc: 0.916, val_acc: 0.900\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cab8e3918e5645859deb26d3e68e2a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='steps', max=937, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "steps: 100%|██████████| 235/235 [00:49<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\r",
      "global_step: 3000, tr_loss: 0.168, val_loss: 0.273, tr_acc: 0.934, val_acc: 0.899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "steps: 100%|██████████| 235/235 [00:52<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step: 3500, tr_loss: 0.171, val_loss: 0.264, tr_acc: 0.932, val_acc: 0.901\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "148f570b07e34779a5d2531424b4304f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='steps', max=937, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "steps: 100%|██████████| 235/235 [00:51<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\r",
      "global_step: 4000, tr_loss: 0.136, val_loss: 0.269, tr_acc: 0.949, val_acc: 0.899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "steps: 100%|██████████| 235/235 [00:51<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step: 4500, tr_loss: 0.140, val_loss: 0.292, tr_acc: 0.947, val_acc: 0.901\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm_notebook(range(model_config.epochs), desc='epochs'):\n",
    "\n",
    "    tr_loss = 0\n",
    "    tr_acc = 0\n",
    "\n",
    "    model.train()\n",
    "    for step, mb in tqdm_notebook(enumerate(tr_dl), desc='steps', total=len(tr_dl)):\n",
    "        x_mb, y_mb = map(lambda elm: elm.to(device), mb)\n",
    "        opt.zero_grad()\n",
    "        y_hat_mb = model(x_mb)\n",
    "        mb_loss = loss_fn(y_hat_mb, y_mb)\n",
    "        mb_loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mb_acc = acc(y_hat_mb, y_mb)\n",
    "\n",
    "        tr_loss += mb_loss.item()\n",
    "        tr_acc += mb_acc.item()\n",
    "        \n",
    "#         writer.add_scalars('train', {'loss': tr_loss / (step + 1),\n",
    "#                             'acc': tr_acc / (step + 1)}, epoch * len(tr_dl) + step)\n",
    "\n",
    "        if (epoch * len(tr_dl) + step) % model_config.summary_step == 0:\n",
    "            tr_summary = {'loss': tr_loss / (step+1), 'acc': tr_acc / (step+1)}\n",
    "            val_summary = evaluate(model, val_dl, {'loss': loss_fn, 'acc': acc}, device)\n",
    "#             writer.add_scalars('val', {'loss': val_summary['loss'],\n",
    "#                                         'acc': val_summary['acc']}, epoch * len(tr_dl) + step)\n",
    "            tqdm.write('global_step: {:3}, tr_loss: {:.3f}, val_loss: {:.3f}, '\n",
    "                       'tr_acc: {:.3f}, val_acc: {:.3f}'.format(epoch * len(tr_dl) + step,\n",
    "                                                                tr_summary['loss'], val_summary['loss'],\n",
    "                                                               tr_summary['acc'], val_summary['acc']))\n",
    "            is_best = val_summary['loss'] < best_val_loss\n",
    "            model.train()\n",
    "\n",
    "\n",
    "        if is_best:\n",
    "            state = {'epoch': epoch + 1,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'opt_state_dict': opt.state_dict()}\n",
    "            summary = {'train': tr_summary, 'validation': val_summary}\n",
    "            summary_manager.update(summary)\n",
    "            summary_manager.save('summary_etri.json')\n",
    "            checkpoint_manager.save_checkpoint(state, 'best_etri.tar')\n",
    "\n",
    "            best_val_loss = val_summary['loss']\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![train_loss](./figs/train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![validation_loss](./figs/val_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "steps: 100%|██████████| 391/391 [01:26<00:00,  4.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.267, acc: 89.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# model (restore)\n",
    "checkpoint_manager = CheckpointManager(model_dir)\n",
    "checkpoint = checkpoint_manager.load_checkpoint('best_etri.tar')\n",
    "config = BertConfig(ptr_config.config)\n",
    "\n",
    "model = SentenceClassifier(\n",
    "    config, \n",
    "    num_classes = model_config.num_classes,\n",
    "    vocab = preprocessor.vocab)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device)\n",
    "\n",
    "# evaluation\n",
    "test_ds = Corpus(data_config.test, preprocessor.preprocess)\n",
    "test_dl = DataLoader(test_ds, batch_size=model_config.batch_size, num_workers=4)\n",
    "\n",
    "summary = evaluate(model, test_dl, {'loss': loss_fn, 'acc': acc}, device)\n",
    "\n",
    "summary_manager.load('summary_etri.json')\n",
    "summary_manager.update({'etri': summary})\n",
    "summary_manager.save('summary_etri.json')\n",
    "\n",
    "print('loss: {:.3f}, acc: {:.2%}'.format(summary['loss'], summary['acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real Data - Clementine, 2004"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./figs/clementine.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 0, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = [\n",
    "    '모니터도 울고 외장하드도 울고 숨어있던 바이러스도 울었다',\n",
    "    '당신이 이 영화를 보지 않았다면 아직 살아있을 이유 하나를 간직하고 있는 것이다',\n",
    "    '신은 인간을 만들었고, 인간은 클레멘타인을 만들었다',\n",
    "    '사상최악의 영화! 빵점줄수없는게 아쉽다.. 보다가 돌렸다 그냥 어휴 눈썩어',\n",
    "    '죽여버려 ...재밋다해서 봤는데 붇여버려 썸녀랑 사이 어색해졌다',\n",
    "    '추천해준 친구놈의 목을따러 가야겠습니다'\n",
    "]\n",
    "\n",
    "with torch.no_grad():\n",
    "    example_input = [preprocessor.preprocess(seq) for seq in example]\n",
    "    example_out = model(torch.tensor(example_input).to(device))\n",
    "    example_pred = example_out.argmax(dim = 1)\n",
    "    \n",
    "example_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
