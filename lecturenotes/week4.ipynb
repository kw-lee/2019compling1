{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DLfS 05. Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 단순한 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4.1 곱셈 계층\n",
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y\n",
    "        dy = dout * self.x\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4.2 덧셈 계층\n",
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220.00000000000003\n",
      "2.2 110.00000000000001 200\n"
     ]
    }
   ],
   "source": [
    "# Example in 5.1\n",
    "# values\n",
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "# 계층들\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# 순전파\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    "\n",
    "print(price)  # 220.0\n",
    "\n",
    "# 역전파\n",
    "dprice = 1\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "print(dapple, dapple_num, dtax)  # 2.2 110.0 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715.0000000000001\n",
      "110.00000000000001 2.2 3.3000000000000003 165.0 650\n"
     ]
    }
   ],
   "source": [
    "# Example in 5.2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "\n",
    "# 계층들\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# 순전파\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price)\n",
    "price = mul_tax_layer.forward(all_price, tax)\n",
    "\n",
    "print(price)  # 715.0\n",
    "\n",
    "# 역전파\n",
    "dprice = 1\n",
    "dall_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)\n",
    "dornage, dorange_num = mul_orange_layer.backward(dorange_price)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(dapple_num, dapple, dornage, dorange_num, dtax)\n",
    "# 110.0 2.2 3.3 165.0 650"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On my own example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900\n",
      "60 60 60\n"
     ]
    }
   ],
   "source": [
    "# square?\n",
    "# z = (x+y)^2\n",
    "class SquareLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = x**2\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * 2 * self.x\n",
    "        return dx\n",
    "    \n",
    "x = 10\n",
    "y = 20\n",
    "\n",
    "\"\"\"\n",
    "x -> \n",
    "     t = x+y -> z = t^2\n",
    "y -> \n",
    "\"\"\"\n",
    "\n",
    "# 계층들\n",
    "add_layer = AddLayer()\n",
    "square_layer = SquareLayer()\n",
    "\n",
    "# 순전파\n",
    "t = add_layer.forward(x, y)\n",
    "z = square_layer.forward(t)\n",
    "print(z) # (10+20)^2 = 900\n",
    "\n",
    "# 역전파\n",
    "dz = 1\n",
    "dt = square_layer.backward(dz)\n",
    "dx, dy = add_layer.backward(dt)\n",
    "print(dx, dy, dt) # dz/dx = dz/dy = 2(x+y) = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 활성화 함수 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 5.5.1 ReLU 계층\n",
    "\"\"\"\n",
    "y = x (x > 0)\n",
    "    0 (x <= 0)\n",
    "∂y/∂x  = 1 (x > 0)\n",
    "         0 (x <= 0)\n",
    "\n",
    "ReLU의 계산 그래프\n",
    "if x > 0\n",
    "x     → relu → y\n",
    "∂L/∂y ← relu ← ∂L/∂y\n",
    "\n",
    "if x <= 0\n",
    "x → relu → y\n",
    "0 ← relu ← ∂L/∂y\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "# 5.5.2 Sigmoid 계층\n",
    "\"\"\"\n",
    "y = 1 / (1 + exp(-x))\n",
    "\n",
    "시그모이드의 계산 그래프\n",
    "x → × → exp → + → / → y\n",
    "-1↗         1↗\n",
    "\n",
    "1단계\n",
    "'/'노드\n",
    "y = 1/x\n",
    "∂y/∂x = -1/x^2 = -y²\n",
    "상류에서 흘러온 값에 -y^2(순전파의 출력을 제곱하고 마이너스)을 곱해서 하류로 전달 : -∂L/∂y*y²\n",
    "\n",
    "2단계\n",
    "'+'노드\n",
    "상류의 값을 그대로 하류로 전달 : -∂L/∂y*y²\n",
    "\n",
    "3단계\n",
    "'exp'노드\n",
    "y = exp(x)\n",
    "∂y/∂x = exp(x)\n",
    "상류의 값에 순전파 때의 출력(이 경우엔 exp(-x))을 곱해 하류로 전달 : -∂L/∂y*y²*exp(-x)\n",
    "\n",
    "4단계\n",
    "'×'노드\n",
    "순전파 때의 값을 서로 바꿔 곱함(여기서는 * -1) : ∂L/∂y*y²*exp(-x)\n",
    "∂L/∂y*y^2*exp(-x)는 정리하면 ∂L/∂y*y(1-y)가 된다.(순전파의 출력만으로 계산할 수 있다)\n",
    "\n",
    "정리\n",
    "x            → sigmoid → y\n",
    "∂L/∂y*y(1-y) ← sigmoid ← ∂L/∂y\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  -0.5]\n",
      " [-2.   3. ]]\n",
      "[[False  True]\n",
      " [ True False]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n[[False  True]\\n [ True False]]\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1.0, -0.5], [-2.0, 3.0]])\n",
    "print(x)\n",
    "\"\"\"\n",
    "[[ 1.  -0.5]\n",
    " [-2.   3. ]]\n",
    "\"\"\"\n",
    "\n",
    "mask = (x <= 0)\n",
    "print(mask)\n",
    "\"\"\"\n",
    "[[False  True]\n",
    " [ True False]]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Affine/Softmax 계층 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "(2, 3)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "# 5.6.1 Affine 계층\n",
    "\"\"\"\n",
    "신경망의 순전파에서는 가중치 신호의 총합을 계산하기 때문에 행렬의 내적을 사용했다.(3.3 참고)\n",
    "\"\"\"\n",
    "X = np.random.rand(2)     # 입력\n",
    "W = np.random.rand(2, 3)  # 가중치\n",
    "B = np.random.rand(3)     # 편향\n",
    "\n",
    "print(X.shape)  # (2,)\n",
    "print(W.shape)  # (2, 3)\n",
    "print(B.shape)  # (3,)\n",
    "\n",
    "Y = np.dot(X, W) + B\n",
    "# 신경망의 순전파 때 수행하는 행렬의 내적은 기하학에서는 어파인 변환이라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affine 계층의 계산 그래프\n",
    "X, W, B는 행렬(다차원 배열)\n",
    "\n",
    "1. X ↘    X·W\n",
    "       dot → + → Y\n",
    "2. W ↗ 3.B ↗\n",
    "\n",
    "1. ∂L/∂X = ∂L/∂Y·W^T\n",
    "   (2,)    (3,)  (3,2)\n",
    "2. ∂L/∂W = X^T·∂L/∂Y\n",
    "   (2,3)  (2,1)(1,3)\n",
    "3. ∂L/∂B = ∂L/∂Y\n",
    "   (3,)    (3,)\n",
    "W^T : W의 전치행렬(W가 (2,3)이라면 W^T는(3,2)가 된다.)\n",
    "X = (x0, x1, x2, ..., xn)\n",
    "∂L/∂X = (∂L/∂x0, ∂L/∂x1, ∂L/∂x2, ..., ∂L/∂xn)\n",
    "따라서 X와 ∂L/∂X의 형상은 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0]\n",
      " [10 10 10]]\n",
      "[[ 1  2  3]\n",
      " [11 12 13]]\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "[5 7 9]\n"
     ]
    }
   ],
   "source": [
    "# 5.6.2 배치용 Affine 계층\n",
    "\"\"\"\n",
    "입력 데이터로 X 하나만이 아니라 데이터 N개를 묶어 순전파하는 배치용 계층을 생각\n",
    "\n",
    "배치용 Affine 계층의 계산 그래프\n",
    "X의 형상이 (N,2)가 됨.\n",
    "\n",
    "1. ∂L/∂X = ∂L/∂Y·W^T\n",
    "   (N,2)   (N,3) (3,2)\n",
    "2. ∂L/∂W = X^T·∂L/∂Y\n",
    "   (2,3)  (2,N)(N,3)\n",
    "3. ∂L/∂B = ∂L/∂Y의 첫 번째 축(0축, 열방향)의 합.\n",
    "   (3,)    (N,3)\n",
    "\n",
    "편향을 더할 때에 주의해야 한다. 순전파 때의 편향 덧셈은 X·W에 대한 편향이\n",
    "각 데이터에 더해진다. 예를 들어 N=2일 경우 편향은 두 데이터 각각에 더해진다.\n",
    "\"\"\"\n",
    "if __name__ == '__main__':\n",
    "    X_dot_W = np.array([[0, 0, 0], [10, 10, 10]])\n",
    "    B = np.array([1, 2, 3])\n",
    "    print(X_dot_W)\n",
    "    \"\"\"\n",
    "    [[ 0  0  0]\n",
    "     [10 10 10]]\n",
    "    \"\"\"\n",
    "    print(X_dot_W + B)\n",
    "    \"\"\"\n",
    "    [[ 1  2  3]\n",
    "     [11 12 13]]\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    순전파의 편향 덧셈은 각각의 데이터에 더해지므로\n",
    "    역전파 때는 각 데이터의 역전파 값이 편향의 원소에 모여야 한다.\n",
    "    \"\"\"\n",
    "    dY = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "    print(dY)\n",
    "    \"\"\"\n",
    "    [[1 2 3]\n",
    "     [4 5 6]]\n",
    "    \"\"\"\n",
    "    dB = np.sum(dY, axis=0)\n",
    "    print(dB)  # [5 7 9]\n",
    "    \"\"\"\n",
    "    데이터가 두 개일 때 편향의 역전파는 두 데이터에 대한 미분을 데이터마다\n",
    "    더해서 구한다.\n",
    "    np.sum()에서 0번째 축(데이터를 단위로 한 축. axis=0)에 대해서 합을 구한다.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layers\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5.6.3 Softmax-with-Loss 계층\n",
    "\"\"\"\n",
    "소프트맥스 계층 : 입력 값을 정규화(출력의 합이 1이 되도록 변경)하여 출력\n",
    "학습과 추론 중 학습에서 주로 사용\n",
    "소프트맥스 계층과 손실 함수(교차 엔트로피 오차)를 포함해 계산 그래프를 그림\n",
    "자세한 역전파 계산은 부록A 참고.\n",
    "\n",
    "간소화한 Softmax-with-Loss계층의 계산 그래프\n",
    "a1   →    |         | → y1 → |         |\n",
    "y1 - t1 ← |         |   t1 ↗  | Cross   |\n",
    "a2   →    | Softmax | → y2 → | Entropy | → L\n",
    "y2 - t2 ← |         |   t2 ↗  | Error   | ← 1\n",
    "a3   →    |         | → y3 → |         |\n",
    "y3 - t3 ←               t3 ↗\n",
    "입력 : (a1, a2, a3)\n",
    "정규화된 출력 : (y1, y2, y3)\n",
    "정답 레이블 (t1, t2, t3)\n",
    "손실 : L\n",
    "\n",
    "역전파로 Softmax 계층의 출력과 정답 레이블의 차분 값\n",
    "(y1 - t1, y2 - t2, y2 - t2)이 전달됨.\n",
    "이는 교차 엔트로피 오차 함수가 그렇게 설계되었기 때문.\n",
    "항등 함수의 손실 함수로는 평균 제곱 오차를 사용하는데,\n",
    "그럴 경우 역전파의 결과가 (y1 - t1, y2 - t2, y2 - t2)로 말끔히 떨어짐.\n",
    "\n",
    "ex) 정답 레이블 t = (0, 1, 0) 일 때,\n",
    "소프트맥스가 (0.3, 0.2, 0.5)를 출력했다고 할 때, 소프트맥스 계층의 역전파는\n",
    "(0.3, -0.8, 0.5)로 앞 계층에 큰 오차를 전파하게 됨\n",
    "소프트맥스가 (0.01, 0.99, 0.)을 출력했다면 역전파는 (0.01, -0.01, 0)\n",
    "으로 오차가 작아짐\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# yk = exp(ak) / ∑(i=1 to n)(exp(ai))\n",
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a - c)  # 오버플로 대책\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7  # 0일때 -무한대가 되지 않기 위해 작은 값을 더함\n",
    "    return -np.sum(t * np.log(y + delta))\n",
    "\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None  # 손실\n",
    "        self.y = None     # softmax의 출력\n",
    "        self.t = None     # 정답 레이블(원-핫 벡터)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)  # 3.5.2, 4.2.2에서 구현\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = self.y - self.t / batch_size\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007620616629495912\n",
      "[0.00090496 0.65907491 0.00668679]\n",
      "5.0076057626568575\n",
      "[ 9.04959183e-04 -3.26646539e-01  9.92408247e-01]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    swl = SoftmaxWithLoss()\n",
    "    a = np.array([1, 8, 3])   # 비슷하게 맞춤\n",
    "    t = np.array([0, 1, 0])\n",
    "    print(swl.forward(a, t))  # 0.0076206166295\n",
    "    print(swl.backward())     # [ 0.00090496  0.65907491  0.00668679]\n",
    "\n",
    "    a = np.array([1, 3, 8])   # 오차가 큼\n",
    "    print(swl.forward(a, t))  # 5.00760576266\n",
    "    print(swl.backward())   # [  9.04959183e-04 -3.26646539e-01 9.92408247e-01]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forwardpropagation, Backpropagation and Gradient Descent with PyTorch\n",
    "\n",
    "https://www.deeplearningwizard.com/deep_learning/boosting_models_pytorch/forwardpropagation_backpropagation_gradientdescent/\n",
    "\n",
    "or\n",
    "\n",
    "[rel path](../../deep-learning-wizard/docs/deep_learning/boosting_models_pytorch/forwardpropagation_backpropagation_gradientdescent.ipynb)\n",
    "\n",
    "(오타가 있어 코드를 직접 수정했음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
