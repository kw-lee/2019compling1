{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "[An overview of gradient descent optimization algorithms](http://ruder.io/optimizing-gradient-descent/)\n",
    "\n",
    "---\n",
    "\n",
    "# Gradient descent variants\n",
    "\n",
    "## Batch Gradient Descent\n",
    "\n",
    "$$\\theta = \\theta - \\eta \\cdot \\nabla_\\theta J( \\theta)$$\n",
    "\n",
    " ```python   \n",
    "for i in range(nb_epochs):\n",
    "      params_grad = evaluate_gradient(loss_function, data, params)\n",
    "      params = params - learning_rate * params_grad\n",
    "```\n",
    "\n",
    "## SGD (Stochastic Gradient Descent)\n",
    "\n",
    "$$\\theta = \\theta - \\eta \\cdot \\nabla_\\theta J( \\theta; x^{(i)}; y^{(i)})$$\n",
    "\n",
    "where $(x^{(i)}, y^{(i)})$'s are randomly selected\n",
    "\n",
    "```python\n",
    "for i in range(nb_epochs):\n",
    "  np.random.shuffle(data)\n",
    "  for example in data:\n",
    "    params_grad = evaluate_gradient(loss_function, example, params)\n",
    "    params = params - learning_rate * params_grad\n",
    "```\n",
    "\n",
    "## Mini-Batch Gradient Descent\n",
    "\n",
    "$$\\theta = \\theta - \\eta \\cdot \\nabla_\\theta J( \\theta; x^{(i:i+n)}; y^{(i:i+n)})$$\n",
    "\n",
    "```python    \n",
    "for i in range(nb_epochs):\n",
    "  np.random.shuffle(data)\n",
    "  for batch in get_batches(data, batch_size=50):\n",
    "    params_grad = evaluate_gradient(loss_function, batch, params)\n",
    "    params = params - learning_rate * params_grad\n",
    "```\n",
    "\n",
    "Vanilla mini-batch gradient descent, however, does not guarantee good convergence, but offers a few challenges that need to be addressed:\n",
    "\n",
    "- Choosing a proper learning rate can be difficult. A learning rate that is too small leads to painfully slow convergence, while a learning rate that is too large can hinder convergence and cause the loss function to fluctuate around the minimum or even to diverge.\n",
    "- Learning rate schedules [[1]](http://ruder.io/optimizing-gradient-descent/index.html#fn1) try to adjust the learning rate during training by e.g. annealing, i.e. reducing the learning rate according to a pre-defined schedule or when the change in objective between epochs falls below a threshold. These schedules and thresholds, however, have to be defined in advance and are thus unable to adapt to a dataset's characteristics [[2]](http://ruder.io/optimizing-gradient-descent/index.html#fn2).\n",
    "- Additionally, the same learning rate applies to all parameter updates. If our data is sparse and our features have very different frequencies, we might not want to update all of them to the same extent, but perform a larger update for rarely occurring features.\n",
    "- Another key challenge of minimizing highly non-convex error functions common for neural networks is avoiding getting trapped in their numerous suboptimal local minima. Dauphin et al. [[3]](http://ruder.io/optimizing-gradient-descent/index.html#fn3) argue that the difficulty arises in fact not from local minima but from saddle points, i.e. points where one dimension slopes up and another slopes down. These saddle points are usually surrounded by a plateau of the same error, which makes it notoriously hard for SGD to escape, as the gradient is close to zero in all dimensions.\n",
    "\n",
    "# GD with Momentum\n",
    "\n",
    "It does this by adding a fraction γ of the update vector of the past time step to the current update vector:\n",
    "\n",
    "$$\\begin{aligned}  v_t &= \\gamma v_{t-1} + \\eta \\nabla_\\theta J( \\theta) \\\\  \\theta &= \\theta - v_t  \\end{aligned}$$\n",
    "\n",
    "As a result, we gain faster convergence and reduced oscillation.\n",
    "\n",
    "## NAG (Nesterov accelerated gradient)\n",
    "\n",
    "Computing θ−γvt−1 thus gives us an approximation of the next position of the parameters (the gradient is missing for the full update), a rough idea where our parameters are going to be. We can now effectively look ahead by calculating the gradient not w.r.t. to our current parameters θ but w.r.t. the approximate future position of our parameters:\n",
    "\n",
    "$$\\begin{aligned}  v_t &= \\gamma v_{t-1} + \\eta \\nabla_\\theta J( \\theta - \\gamma v_{t-1} ) \\\\  \\theta &= \\theta - v_t  \\end{aligned}$$\n",
    "\n",
    "Again, we set the momentum term γ to a value of around 0.9. While Momentum first computes the current gradient (small blue vector in Image 4) and then takes a big jump in the direction of the updated accumulated gradient (big blue vector), NAG first makes a big jump in the direction of the previous accumulated gradient (brown vector), measures the gradient and then makes a correction (red vector), which results in the complete NAG update (green vector). This anticipatory update prevents us from going too fast and results in increased responsiveness, which has significantly increased the performance of RNNs on a number of tasks [[7]](http://ruder.io/optimizing-gradient-descent/index.html#fn7).\n",
    "\n",
    "![](http://ruder.io/content/images/2016/09/nesterov_update_vector.png)\n",
    "\n",
    "Image 4: Nesterov update (Source: [G. Hinton's lecture 6c](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf))\n",
    "\n",
    "# Adaptive Gradient Descent\n",
    "\n",
    "## Adagrad\n",
    "\n",
    "## Adadelta\n",
    "\n",
    "## RMSprop\n",
    "\n",
    "# AdaGrad + Momentum\n",
    "\n",
    "## Adam (Adaptive Moment Estimation)\n",
    "\n",
    "## AdaMax\n",
    "\n",
    "## Nadam\n",
    "\n",
    "## AMSGrad\n",
    "\n",
    "[](https://www.notion.so/0e9b4954aee44412a86514e2c73a0628#05b181ae611a4857a690cc06c509012d)\n",
    "\n",
    "mage 5: SGD optimization on loss surface contours\n",
    "\n",
    "[](https://www.notion.so/0e9b4954aee44412a86514e2c73a0628#b337043dd5354a33b448036ff4a0f3d8)\n",
    "\n",
    "Image 6: SGD optimization on saddle point\n",
    "\n",
    "# Look-Ahead Algorithm\n",
    "\n",
    "[lonePatient/lookahead_pytorch](https://github.com/lonePatient/lookahead_pytorch)\n",
    "\n",
    "[New Deep Learning Optimizer, Ranger: Synergistic combination of RAdam + LookAhead for the best of both.](https://medium.com/@lessw/new-deep-learning-optimizer-ranger-synergistic-combination-of-radam-lookahead-for-the-best-of-2dc83f79a48d)\n",
    "\n",
    "# Which optimizer to use?\n",
    "\n",
    "So, which optimizer should you now use? If your input data is sparse, then you likely achieve the best results using one of the adaptive learning-rate methods. An additional benefit is that you won't need to tune the learning rate but likely achieve the best results with the default value.\n",
    "\n",
    "In summary, RMSprop is an extension of Adagrad that deals with its radically diminishing learning rates. It is identical to Adadelta, except that Adadelta uses the RMS of parameter updates in the numinator update rule. Adam, finally, adds bias-correction and momentum to RMSprop. Insofar, RMSprop, Adadelta, and Adam are very similar algorithms that do well in similar circumstances. Kingma et al. [[14:1]](http://ruder.io/optimizing-gradient-descent/index.html#fn14) show that its bias-correction helps Adam slightly outperform RMSprop towards the end of optimization as gradients become sparser. Insofar, Adam might be the best overall choice.\n",
    "\n",
    "Interestingly, many recent papers use vanilla SGD without momentum and a simple learning rate annealing schedule. As has been shown, SGD usually achieves to find a minimum, but it might take significantly longer than with some of the optimizers, is much more reliant on a robust initialization and annealing schedule, and may get stuck in saddle points rather than local minima. Consequently, if you care about fast convergence and train a deep or complex neural network, you should choose one of the adaptive learning rate methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [derivative_gradient](../../deep-learning-wizard/docs/deep_learning/boosting_models_pytorch/derivative_gradient_jacobian.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [optimizer](../../deep-learning-wizard/docs/deep_learning/boosting_models_pytorch/optimizer.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameter tuning을 위한 optimization\n",
    "\n",
    "* GD: data size가 커지면 사실상 불가능\n",
    "* SGD: 느리고 요동침\n",
    "    * SGD Momentum: SGD가 요동치는 것을 막는다\n",
    "    * SGD N: 미래 관점에서 방향을 결정\n",
    "* Ada-: Adaptive learning rates\n",
    "    * Adagrad: $G$를 도입해서 step size를 결정\n",
    "    * $G$가 너무 커지면 stuck이 걸리기 때문에 RAda- 등이 제안됨\n",
    "* Adam: RMSProp + Momentum\n",
    "    * RAdam: https://medium.com/@lessw/new-state-of-the-art-ai-optimizer-rectified-adam-radam-5d854730807b\n",
    "* Lookahead: https://medium.com/@lessw/new-deep-learning-optimizer-ranger-synergistic-combination-of-radam-lookahead-for-the-best-of-2dc83f79a48d\n",
    "http://ruder.io/optimizing-gradient-descent/ 참조\n",
    "* 합해서 Ranger 도 있음\n",
    "\n",
    "note에 안돌아가는 부분들이 있는데 알아서 고쳐보기..!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gpu는 optimizer에 작동하지 않음 (gradient 계산에만 쓰이니까)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Learning Rate Scheduling](../../deep-learning-wizard/docs/deep_learning/boosting_models_pytorch/lr_scheduling.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch에 추적할 수 있는 패키지가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Weight Initialization](../../deep-learning-wizard/docs/deep_learning/boosting_models_pytorch/weight_initialization.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialize를 잘 해야 학습이 잘 된다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
